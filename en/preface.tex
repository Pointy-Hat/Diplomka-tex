\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\label{intro}

In this thesis we propose and try to build a new tool for automated theorem proving. The theorem provers, that is algorithms that given a theory and a conjecture find a formal proof, mostly use syntactic rules to manipulate formulas. This approach is very different from how human mathematicians think about mathematics. When a human is working with a theory, they usually do so with some sort of mental images of the structures that satisfy it. We aim to build these models for automatic provers, thus giving them the ability to also leverage the semantic meaning of the given theory. These models could be used as an oracle that guesses validity of given sentences by trying whether or not do they hold in the trained models.

Another crucial aspect of intuitive understanding of structures is the ability to naturally extend them. Most structures have extensions that can be called "algebraic", i.e. those that arise when we add solutions to an equation that has no solution in the structure itself. Best known examples are algebraic extensions of rings, for example using the equation $x^2-2=0$ in $\mathbb{Z}$ or $x^2+1=0$ in $\mathbb{R}$.

The structures that mathematicians work with, however, can be quite complex. So complex in fact, that handcrafting a model that the computer can work with gets quite challenging. That is why we rely on the machine learning, namely the universal property of the neural networks - their ability to approximate any continuous function with arbitrary precision. Neural network learning, however, needs to work with differentiable functions, that do not exist in most structures. To enable their usage we choose a representation of the structure elements in $\mathbb{R}^n$, which we will call \textit{grounding}. Here we will use exclusively handpicked groundings that give us better insight into the performance of the model.\\

The main subject and the main contribution of this thesis is the description and testing of the network architecture (inspired by \cite{serafini}) that enables the networks approximating the structure functions to learn using the propositions that are true in the structures. The description of this architecture can be found in \Autoref{chapter:implementation}. Using with the framework Tensorflow (\cite{tf}) we have built neural models and extensions of groups, namely $\mathbb{Z}_n$ and $S_n$. Results of these experiments can be found in \Autoref{chapter:results}.

Another minor contribution is the proof that the algebraic extensions of groups are always possible (in \Autoref{section:groups}). It is very likely that this was proven before, but we were unable to locate any such proof, thus we present this proof as our own.