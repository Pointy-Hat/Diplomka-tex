\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\label{intro}

In the world of automated theorem proving, there had always been a disconnect between how computers and humans do mathematics. One of the reasons why computers have not been able to emulate human reasoning is the human capacity for intuition. When working with a well known structure (e.g. field of real numbers), they operate with its mental image. Therefore they can usually correctly guess whether or not a given sentence will hold or not. This assessment is possible even if the human can not prove the sentence using a formal proof system. Computers have so far been unable to do this estimation very well, mostly because they operate only with axioms of the structures and have no such image.

In this thesis, we do first experiments with trying to build such mental image, so that it can later be used to do these estimations. The ultimate aim is to have an oracle that guesses validity of given sentences based on a number of pre-trained models of the theory. 

Another crucial aspect of intuitive understanding of structures is the ability to naturally extend them. Most structures have extensions that can be called "algebraic", i.e. those that arise when we add solutions to an equation that has no solution in the structure itself. Best known examples are algebraic extensions of rings, for example using the equation $x^2-2=0$ in $\mathbb{Z}$ or $x^2+1=0$ in $\mathbb{R}$.

The structures that mathematicians work with, however, can be quite complex. So complex in fact, that handcrafting a model that the computer can work with gets quite challenging. That is why we rely on the machine learning, namely we rely the universal property of the neural networks - their ability to approximate any continuous function with arbitrary precision. Neural network learning, however, needs to work with differentiable functions, that do not exist in most structures. To enable their usage we choose a representation of the structure elements in $\mathbb{R}^n$, which we will call \textit{grounding}. Here we will use exclusively handpicked groundings that give us better insight into the performance of the model.

We try to teach the networks using the propositions that are true/false in the structures. The details can be found in \Autoref{chapter:implementation}. Using this approach with the framework Tensorflow (\cite{tf}) we have built neural models and extensions of groups, namely $\mathbb{Z}_n$ and $S_n$. Results of these experiments can be found in \Autoref{chapter:results}. However, we have so far not tried any experimentation on other structures. Experiments with other structures are possible, however beyond the scope of this thesis. 