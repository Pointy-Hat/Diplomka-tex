\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\label{intro}

In the world of automated theorem proving, there had always been a disconnect between how computers and humans do mathematics. One of the reasons why computers have not been able to emulate human reasoning is the human capacity for intuition. When working with a well known structure (e.g. field of real numbers), they operate with its mental image. Therefore they can usually correctly guess whether or not will a given sentence hold or not. This assessment is possible even if the human can not prove the sentence using a formal proof system. Computers have so far been unable to do this estimation very well, mostly because they operate only with axioms of the structures and have no such image.

We try to build this image, so that it can later be used to do these estimations. The ultimate aim is to have an oracle that guesses "truthiness" of given sentences based on a number of pre-trained models of the theory. 

Another crucial aspect of intuitive understanding of structures is the ability to naturally extend them. Most structures have extensions that can be called "algebraic", i.e. those that arise when we add solutions to an equation that has no solution in the structure itself. Best known examples are algebraic extensions of rings, for example using the equation $x^2-2=0$ in $\mathbb{Z}$ or $x^2+1=0$ in $\mathbb{R}$.

The structures that mathematicians work with, however, can be quite complex. So complex in fact, that handcrafting a model that the computer can work with gets quite challenging. That is why we want to use machine learning, namely we rely the universal property of the neural networks - their ability to approximate any continuous function with arbitrary precision. Neural network learning, however, needs to work with differentiable functions, that do not exist in most structures. To enable their usage we choose a representation of the structure elements in $\mathbb{R}^n$, which we will call \textit{grounding}. Here we will use exclusively handpicked groundings that give us better insight into the preformance of the model.

We try to teach the networks using the axioms of a structure. The details can be found in \Autoref{chapter:implementation}. Using this construction in the framework Tensorflow (\cite{tf}) we have built models and extensions of groups, namely $\mathbb{Z}_n$ and $S_n$. Results of this experimentation can be found in \Autoref{chapter:results}. However, we have so far not tried any experimentation on other structures. It would be possible to build such an extension, but that is beyond the scope of this thesis. 