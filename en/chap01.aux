\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{model}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Model theory}{3}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Basic definitions}{3}{subsection.1.1.1}}
\citation{logic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Skolemization}{5}{subsection.1.1.2}}
\citation{logic}
\citation{model}
\citation{neural}
\citation{relu1}
\citation{relu2}
\citation{relu3}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Neural networks}{7}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is a neural network}{7}{subsection.1.2.1}}
\citation{neural}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Neurons in a layer\relax }}{8}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{nndiagram1}{{1.1}{8}{Neurons in a layer\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces ReLU\relax }}{8}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Example: XOR}{8}{subsection.1.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces XOR network\relax }}{9}{figure.caption.5}}
\newlabel{xordiagram}{{1.3}{9}{XOR network\relax }{figure.caption.5}{}}
\citation{imagenet_relu}
\citation{lrelu}
\citation{parametric_relu}
\citation{elu}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Other activation functions}{10}{subsection.1.2.3}}
\citation{swish}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Sigmoid and tanh functions\relax }}{11}{figure.caption.6}}
\newlabel{sigmoiddiagram}{{1.4}{11}{Sigmoid and tanh functions\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Learning and optimization}{11}{subsection.1.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces ReLU variations\relax }}{12}{figure.caption.7}}
\newlabel{variousreludiagram}{{1.5}{12}{ReLU variations\relax }{figure.caption.7}{}}
\citation{backprop}
\citation{neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Back-propagation}{13}{subsection.1.2.5}}
\citation{adam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A basic backpropagation algorithm for the most basic feedforward neural networks. We assume that each layer of our network is an affine function $y^{(i)}=f^{(i)}(x^{(i-1)})=W^{(i)}x^{(i-1)}+b^{(i)}$ with activation function $a$ on each element: $x^{(i)}=a(y^{(i)})$. $x$-es here are states between layers and $y$-s are states before applying activation functions for nonlinearity. Here $x^{(0)}=y^{(0)}=$ input and $f^{(1)}$ is the first layer.\relax }}{14}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Adam optimizer}{14}{subsection.1.2.6}}
\newlabel{subsec:adam}{{1.2.6}{14}{Adam optimizer}{subsection.1.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces An example of the symbol to symbol approach. Note that $f^{(3)}$ here can also be the loss function\relax }}{15}{figure.caption.8}}
\newlabel{tfbackpropdiagram}{{1.6}{15}{An example of the symbol to symbol approach. Note that $f^{(3)}$ here can also be the loss function\relax }{figure.caption.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Adam optimizer. This algorithm is exactly like it can be found in the original paper. $g_t^2$ denotes element-wise square. All vector operations are elemet-wise. $f_t$ represents the loss function $f$ realized over the training batch in the step $t$.\relax }}{16}{algorithm.2}}
\newlabel{alg:adam}{{2}{16}{Adam optimizer. This algorithm is exactly like it can be found in the original paper. $g_t^2$ denotes element-wise square. All vector operations are elemet-wise. $f_t$ represents the loss function $f$ realized over the training batch in the step $t$.\relax }{algorithm.2}{}}
\@setckpt{chap01}{
\setcounter{page}{17}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{14}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{12}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@unique}{25}
\setcounter{ALC@line}{13}
\setcounter{ALC@rem}{13}
\setcounter{ALC@depth}{0}
\setcounter{thm}{2}
\setcounter{defn}{14}
\setcounter{section@level}{2}
}
