\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{model}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Model theory}{3}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Basic definitions}{3}{subsection.1.1.1}}
\citation{logic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Skolemization}{5}{subsection.1.1.2}}
\citation{logic}
\citation{model}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Substructures and extensions}{7}{subsection.1.1.3}}
\newlabel{section:extensions}{{1.1.3}{7}{Substructures and extensions}{subsection.1.1.3}{}}
\citation{group}
\citation{group}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Groups and their algebraic extensions}{8}{subsection.1.1.4}}
\newlabel{section:groups}{{1.1.4}{8}{Groups and their algebraic extensions}{subsection.1.1.4}{}}
\newlabel{freegroupdefn}{{23}{8}{}{defn.23}{}}
\citation{neural}
\citation{relu1}
\citation{relu2}
\citation{relu3}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Neural networks}{10}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is a neural network}{10}{subsection.1.2.1}}
\citation{neural}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Neurons in a layer\relax }}{11}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{nndiagram1}{{1.1}{11}{Neurons in a layer\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces ReLU\relax }}{11}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Example: XOR}{11}{subsection.1.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces XOR network\relax }}{12}{figure.caption.5}}
\newlabel{xordiagram}{{1.3}{12}{XOR network\relax }{figure.caption.5}{}}
\citation{imagenet_relu}
\citation{lrelu}
\citation{parametric_relu}
\citation{elu}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Other activation functions}{13}{subsection.1.2.3}}
\citation{swish}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Sigmoid and tanh functions\relax }}{14}{figure.caption.6}}
\newlabel{sigmoiddiagram}{{1.4}{14}{Sigmoid and tanh functions\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces ReLU variations\relax }}{14}{figure.caption.7}}
\newlabel{variousreludiagram}{{1.5}{14}{ReLU variations\relax }{figure.caption.7}{}}
\citation{universal}
\citation{narrownet}
\citation{widthexp}
\citation{narrownet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Universal approximation}{15}{subsection.1.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Learning and optimization}{16}{subsection.1.2.5}}
\citation{backprop}
\citation{neural}
\citation{adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Back-propagation}{17}{subsection.1.2.6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A basic back-propagation algorithm for the most basic feedforward neural networks. We assume that each layer of our network is an affine function $y^{(i)}=f^{(i)}(x^{(i-1)})=W^{(i)}x^{(i-1)}+b^{(i)}$ with activation function $a$ on each element: $x^{(i)}=a(y^{(i)})$. $x$-es here are states between layers and $y$-s are states before applying activation functions for non-linearity. Here $x^{(0)}=y^{(0)}=$ input and $f^{(1)}$ is the first layer.\relax }}{18}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.7}Adam optimizer}{18}{subsection.1.2.7}}
\newlabel{subsec:adam}{{1.2.7}{18}{Adam optimizer}{subsection.1.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces An example of the symbol to symbol approach. Note that $f^{(3)}$ here can also be the loss function\relax }}{19}{figure.caption.8}}
\newlabel{tfbackpropdiagram}{{1.6}{19}{An example of the symbol to symbol approach. Note that $f^{(3)}$ here can also be the loss function\relax }{figure.caption.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Adam optimizer. This algorithm is exactly like it can be found in the original paper. $g_t^2$ denotes element-wise square. All vector operations are element-wise. $f_t$ represents the loss function $f$ realized over the training batch in the step $t$.\relax }}{20}{algorithm.2}}
\newlabel{alg:adam}{{2}{20}{Adam optimizer. This algorithm is exactly like it can be found in the original paper. $g_t^2$ denotes element-wise square. All vector operations are element-wise. $f_t$ represents the loss function $f$ realized over the training batch in the step $t$.\relax }{algorithm.2}{}}
\@setckpt{chap01}{
\setcounter{page}{21}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{7}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{23}
\setcounter{Hfootnote}{7}
\setcounter{bookmark@seq@number}{15}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@unique}{25}
\setcounter{ALC@line}{13}
\setcounter{ALC@rem}{13}
\setcounter{ALC@depth}{0}
\setcounter{thm}{4}
\setcounter{defn}{24}
\setcounter{section@level}{2}
}
