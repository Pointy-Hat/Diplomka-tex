\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
We have seen some promising results with regards to using the neural networks to simulate mathematical models built based on the axioms of those models. We have successfuly learned neural representations of groups, namely the cyclic and symmetric groups. Another focus of the work was building extensions to those models, relying on the learned functions.\\

For every model built here we used a lookup table to learn the composition operation. Even despite the fact that the whole table is not needed (we have had good results even with 10\% of the table missing), prior knowledge of the structure is still required. In order to truly follow the ideas of the model theory, we would need to drop the table altogether. How to do this is currently not known and would be a subject to further experimentation.\\

Another place to improve the method shown here is the grounding, specifically the usage of handpicked representations. This would ideally also be eliminated, since it is another essential part of the model that relies on prior knowledge of the structures. For the self-finding of the groundings we could use recurrent neural networks, which are widely used for feature extraction. Another thing that could improve performance is mutable grounding, i.e. grounding that could change during the learning process to better reflect the structure learned. However, implementation difficulty would be significant.\\

A very large part of this thesis is model extension. Despite initial optimism stemming from the successes of the finite cyclic groups, the results have been rather lackluster. We speculate that this is caused by the fact that we used a lookup table rather than the associativity axiom. The associativity obviously holds in the original universe, since the lookup table had been learned quite efficiently. One way to ensure associativity on the extension as well would be introduction of axioms such as $(h\cdot a)\cdot b = h\cdot (a\cdot b)$ where $h$ is the extension element. However, training for general associativity - i.e. associativity on the whole domain $\mathbb{R}^n$ might slow down the learning process severely.\\

Because the work shown here is very early, there was little focus on the end goal - building an oracle that would gauge the probability that a given sentence is true. This would be a boon to the automated theorem proving community. Current trend is to use machine learning on the sentences themselves, thus skipping the models altogether. This approach has considerable limitations.

Unfortunately, the models building process is very slow (order of hours on a home computer), therefore building an array of models for every problem would be highly inefficient. Automated theorem proving community greatly values the speed of the proof finding, rendering this method rather unwieldy for usage in the state it is in right now. There is however a feasible niche for this model-based oracle in theory building, i.e. expanding a given theory without a set goal. Nonetheless, ATP benchmarks such as TPTP or Mizar do not focus on this part of theorem proving.