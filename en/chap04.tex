\chapter{Comments and discussion}
\label{comments}
The main difference of our approach and the approach of \cite{serafini} is the exclusion of relations. Relations should also be neural networks that take a tuple of elements as input and output a number in $[0,1]$. Relation is, just like an equality between two terms, an atomic formula. In \Autoref{chapter:implementation} we have discussed how to combine different atomic formulas with fuzzy logic operators.

The loss functions shown in \Autoref{chapter:implementation} were based on mean squared difference, which is very inefficient when it comes to 0-1 functions. Here we could use some modifications of cross-entropy function that are robust even with noisy labels (\cite{crossentropy}), since we can not generally be sure of the expected truth value of an atomic subformula. 

Another challenge is posed by the axioms that combine both a relation and a term equality, e.g. $R(\dots)\implies t_1(\dots)=t_2(\dots)$. Here we would have to combine the two different loss functions. One workaround is to use a different loss on the equality subformulas, something that is more related to the cross-entropy.

Our preliminary research showed that to learn each relation we expectedly need both positive and negative examples. We have tried to learn the Sheffer stroke (\cite{sheffer}) - a function on booleans for which all sentences of the type $$((U|(V|W))|((Y|(Y|Y))|((X|V)|((U|X)|(U|X)))))$$ are true for all subformulas $U,V,W,X,Y$. The expected result is the XOR function. The axiomatic approach however led to the network always yielding 1, since there were no examples where it should output 0.\\

Another big challenge in the neural modelling is the choice of grounding. As we have seen with $S_4$, the choice can profoundly impact the learning process. For the models described in this thesis we have used handpicked groundings, but those require prior knowledge of the structure. In order to eliminate this requirement, we need to use a self-found representation of elements. We could do this using recurrent neural networks that have been extensively used in feature extraction, even in logic itself. For example \cite{grounding_wang} have successfuly embedded a knowledge graph (a set of 3-ary relations) to a continuous space, where similar relations are spatially closer to each other. This, under some modification, could prove a promising start for further research.\\

We have also encountered a big problem when training $S_4$ with the matrix grounding, due to the fact that the learned $e$ was not in the original grounding. Although the axiom $a\cdot e=a$ was satisfied for all $a$ in the grounding, $e$ not being an element prevented the inverse function from being an $S\rightarrow S$ function. This leads to the conclusion that we should utilize some penalty for the constants (and maybe functions) that are too far from the given elements. One approach could be to alternate between learning the constants axiomatically and pushing them towards the nearest element ("grid-fitting"). With some fine-tuning of the learning rates this could provide us with a state of equilibrium where a constant settles on an established element. However if the learning rates are configured badly, we could end up in a state where the axiomatic optimizer seeks to abandon an element, but is continually pushed back by the "grid-fitter".

One of the main features of the Adam optimizer used in our experiments is the variable learning rate. Generally speaking, it learns quicker when it is far from the minimum and slower when it is near. We could utilize this and use an inverse learning rate for the "grid-fitting" optimizer. This would lead to the Adam being dominant during the search for the minimum, and when the minimum is closer, the "grid-fitter" would gain precedence and force the constant to be closer to one of the structure elements. This is, however, only speculation.