\chapter{Comments and related research}
\label{comments}
The main difference of our approach and the approach of \cite{serafini} is the exclusion of relations. Relations should also be neural networks that take a tuple of elements as input and output a number in $[0,1]$. Relation is, just like an equality between two terms, an atomic formula. In \Autoref{chapter:implementation} we have discussed how to combine different atomic formulas with fuzzy logic operators.

The loss functions shown in \Autoref{chapter:implementation} were based on mean squared difference, which is very inefficient when it comes to 0-1 functions. Here we could use some modifications of cross-entropy function\footnote{Cross entropy of a non-deterministic function $f:\mathbb{R}^n\rightarrow \{0,1\}$ is defined as $-y\log\widehat{y}-(1-y)\log(1-\widehat{y})$ where $y$ is the probability that $f(x)=1$, while $\widehat{y}$ is the computed probability that $f(x)=1$. Cross-entropy is the lowest when $\widehat{y}=y$ for any given $y$. If $f$ is deterministic, then $y=f(x)$.} (e.g. \cite{crossentropy}).

Another challenge is posed by the axioms that combine both a relation and a term equality, e.g. $R(\dots)\implies t_1(\dots)=t_2(\dots)$. Here we would have to combine the two different loss functions. One workaround is to use a different loss on the equality subformulas, something that is more related to the cross-entropy.

Our preliminary research showed that to learn each relation we expectedly need both positive and negative examples. We have tried to learn the Sheffer stroke (\cite{sheffer}) - a function on booleans for which all sentences of the type $$((U|(V|W))|((Y|(Y|Y))|((X|V)|((U|X)|(U|X)))))$$ are true for all subformulas $U,V,W,X,Y$. The expected result is the XOR function. The axiomatic approach however led to the network always yielding 1, since there were no examples where it should output 0.\\

Another big challenge in the neural modelling is the choice of grounding. As we have seen with $S_4$, the choice can profoundly impact the learning process or even make some extensions impossible. For the models described in this thesis we have used handpicked groundings, but those require prior knowledge of the structure. In order to eliminate this requirement, we would need to use a self-found representation of elements. Representing various entities in vector spaces is a very active area of research. For example \cite{grounding_wang} have successfully embedded a knowledge graph (a set of 3-ary relations) to a continuous space, where similar relations are spatially closer to each other. This, under some modification, could prove a promising start for further research.\\

We have also encountered a big problem when training $S_4$ with the matrix grounding, due to the fact that the learned $e$ was not in the original grounding. Although the axiom $a\cdot e=a$ was satisfied for all $a$ in the grounding, $e$ not being an element prevented the inverse function from being an $S\rightarrow S$ function. This leads to the conclusion that we should modify the loss function to incur some penalty for the constants (and maybe functions) that are too far from the given elements. Other approach could be to alternate between learning the constants axiomatically and "pushing" them towards the nearest element ("grid-fitting"). With some fine-tuning of the learning rates this could end in an equilibrium where the "found" constant settles on an established element. However if the learning rates are configured badly, we could end up in a state where the axiomatic optimizer seeks to abandon an element, but is continually pushed back by the "grid-fitter".

One of the main features of the Adam optimizer used in our experiments is the variable learning rate. Generally speaking, it learns quicker when it is far from the minimum and slower when it is near. We could utilize this and use an inverse learning rate for the "grid-fitting" optimizer. This would lead to the Adam being dominant during the search for the minimum of the axiom loss function, and when the minimum is closer, the "grid-fitter" would gain precedence and force the constant to be closer to one of the structure elements. This is, however, still only speculation.

\section*{Related research and comparison}

At the time of the writing of this thesis, we are unaware of any other model building frameworks that utilize neural networks. In \cite{paradox} they describe a model builder called Paradox, which determines whether or not is a given theory satisfiable. The program is very fast on smaller models, but it struggles with larger structures. It transforms all axioms into clauses\footnote{A clause is a formula of the form $\varphi_1\vee \varphi_2\dots\vee \varphi_n$ where $\varphi_i$ are atomic.} that it then tries to satisfy utilizing a SAT solver. Since SAT is known to be a hard problem, especially on large sets, efficiency of this approach is very dependent on the universe size. For example Paradox is unable to learn the permutation group $S_5$ from the axiomatized multiplication table\footnote{Axiomatized multiplication table is a set of axioms that describe constants $c_{(a,b,c,d)}$ as labels for each element. There are axioms that say that the elements are different: $c_{(a,b,c,d)}\neq c_{(k,l,m,n)}$ if $(a,b,c,d)$ is different from $(k,l,m,n)$. Also, there are axioms that describe the multiplication: $c_{p_1}\circ c_{p_2}=c_{p_1\circ p_2}$}. 