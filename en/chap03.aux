\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Results}{33}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:results}{{3}{33}{Results}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Cyclic groups}{33}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Example network for addition in $\mathbb  {Z}_n$}{33}{subsection.3.1.1}}
\newlabel{section:addition example}{{3.1.1}{33}{Example network for addition in $\mathbb {Z}_n$}{subsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}$\mathbb  {Z}_{10}$}{34}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}$\mathbb  {Z}_{20}$}{35}{subsection.3.1.3}}
\citation{cayley}
\citation{Lingebra}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Infinite group $\mathbb  {Z}$}{36}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Symmetric groups}{36}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}$S_4$ with a basic grounding}{37}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}$S_4$ with matrix grounding}{37}{subsection.3.2.2}}
\citation{Lingebra}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}$S_5$}{39}{subsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Error rate for the \textbf  {discrete} learning of composition, inverse and unit in $\mathbb  {Z}_{10}$ on the testing data. Testing data percentage is 10\%. Each epoch describes one optimization step.\relax }}{40}{figure.caption.15}}
\newlabel{graph:z10_90percent}{{3.1}{40}{Error rate for the \textbf {discrete} learning of composition, inverse and unit in $\mathbb {Z}_{10}$ on the testing data. Testing data percentage is 10\%. Each epoch describes one optimization step.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Error rate for the \textbf  {joint} learning of composition. We see that the error rate is too large for this to be considered a success.\relax }}{40}{figure.caption.16}}
\newlabel{graph:z10_joint}{{3.2}{40}{Error rate for the \textbf {joint} learning of composition. We see that the error rate is too large for this to be considered a success.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Different values of "half" found during different runs. The first run (red) was very peculiar since it had over 2 700 000 epochs, but this representation settled already around epoch 300 000. It is not clear how this interacts with the rest of the elements.\relax }}{41}{figure.caption.17}}
\newlabel{table:z10_half}{{3.3}{41}{Different values of "half" found during different runs. The first run (red) was very peculiar since it had over 2 700 000 epochs, but this representation settled already around epoch 300 000. It is not clear how this interacts with the rest of the elements.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Extension groups generated in two runs (read left to right, top to bottom). The blue elements are supposed to be in the original embedding, i.e. they should be 1...9 ascending with the last 3 elements being 0, "half", 1 respectively. In the first run we see that we have had very low success, even though the first half+half looks promising. The second run had much better success and with the exception of 9 it hit all original elements reasonably well, even those last 3.\relax }}{41}{figure.caption.18}}
\newlabel{table:z10_half_generator}{{3.4}{41}{Extension groups generated in two runs (read left to right, top to bottom). The blue elements are supposed to be in the original embedding, i.e. they should be 1...9 ascending with the last 3 elements being 0, "half", 1 respectively. In the first run we see that we have had very low success, even though the first half+half looks promising. The second run had much better success and with the exception of 9 it hit all original elements reasonably well, even those last 3.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A $\mathbb  {Z}_{20}$ learning run with \textbf  {discrete} approach. These results are from 10\% testing data. We see that the trends we observed in $\mathbb  {Z}_{10}$ continue, and it appears that extra time is not needed. However, in different runs we encountered difficulties with learning the inverse function.\relax }}{42}{figure.caption.19}}
\newlabel{graph:z20_90percent}{{3.5}{42}{A $\mathbb {Z}_{20}$ learning run with \textbf {discrete} approach. These results are from 10\% testing data. We see that the trends we observed in $\mathbb {Z}_{10}$ continue, and it appears that extra time is not needed. However, in different runs we encountered difficulties with learning the inverse function.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A $\mathbb  {Z}_{20}$ learning run with \textbf  {joint} approach. We see that the error rate is far too large to accurately represent the group.\relax }}{42}{figure.caption.20}}
\newlabel{grah:z20_joint}{{3.6}{42}{A $\mathbb {Z}_{20}$ learning run with \textbf {joint} approach. We see that the error rate is far too large to accurately represent the group.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Values for the half in different $\mathbb  {Z}_{20}$ runs. Once again, the reason for why the red one is so different is unknown. The tendency towards 0.5 instead of 10.5 can be attributed to the fact that the initial parameters have been restricted to avoid overflow errors.\relax }}{43}{figure.caption.21}}
\newlabel{table:z20_half}{{3.7}{43}{Values for the half in different $\mathbb {Z}_{20}$ runs. Once again, the reason for why the red one is so different is unknown. The tendency towards 0.5 instead of 10.5 can be attributed to the fact that the initial parameters have been restricted to avoid overflow errors.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces An example of a group generated from the "half" element. We see that there were no problems with addition of the half, except the modulus is not applied correctly.\relax }}{43}{figure.caption.22}}
\newlabel{graph:z20_half_generator}{{3.8}{43}{An example of a group generated from the "half" element. We see that there were no problems with addition of the half, except the modulus is not applied correctly.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces $\mathbb  {Z}$ as an infinite group. Because of limitations of the software, we only trained on the interval $[-10000,10000]$ (hence the trained group is not really infinite). There was no testing set, but the network seemed to generalize well even outside of the training interval. Examples are computed $11000^{-1}=-11001.614$ or $15000+(-12000)=2999.9941$.\relax }}{44}{figure.caption.23}}
\newlabel{graph:z_inf}{{3.9}{44}{$\mathbb {Z}$ as an infinite group. Because of limitations of the software, we only trained on the interval $[-10000,10000]$ (hence the trained group is not really infinite). There was no testing set, but the network seemed to generalize well even outside of the training interval. Examples are computed $11000^{-1}=-11001.614$ or $15000+(-12000)=2999.9941$.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces One run of \textbf  {discrete} learning the $S_4$ with the basic grounding. This graph shows error rates on 10\% testing data. We see that the training is expectedly much slower than in the cyclic groups.\relax }}{44}{figure.caption.24}}
\newlabel{graph:s4_basic}{{3.10}{44}{One run of \textbf {discrete} learning the $S_4$ with the basic grounding. This graph shows error rates on 10\% testing data. We see that the training is expectedly much slower than in the cyclic groups.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces A \textbf  {joint} learning run of $S_4$ with basic grounding. We see that the differences from the \textbf  {discrete} approach all but disappear.\relax }}{45}{figure.caption.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Units learned in $S_4$ with basic grounding using \textbf  {discrete} approach. The expected output was $[0,1,2,3]$. As we see, they are very precise.\relax }}{45}{figure.caption.26}}
\newlabel{table:s4_unit_basic}{{3.12}{45}{Units learned in $S_4$ with basic grounding using \textbf {discrete} approach. The expected output was $[0,1,2,3]$. As we see, they are very precise.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces $h$-s found in several runs of the \textbf  {discrete} approach. As expected, they do not look like anything.\relax }}{45}{figure.caption.27}}
\newlabel{table:s4_half_basic}{{3.13}{45}{$h$-s found in several runs of the \textbf {discrete} approach. As expected, they do not look like anything.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces $h$ composed with itself several times. $h^2$ and $h^6$ should both be $[1,0,2,3]$. $h^4$ should be $[0,1,2,3]$.\relax }}{45}{figure.caption.28}}
\newlabel{table:s4_half_basic_gen}{{3.14}{45}{$h$ composed with itself several times. $h^2$ and $h^6$ should both be $[1,0,2,3]$. $h^4$ should be $[0,1,2,3]$.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces $S_4$ with matrix grounding using \textbf  {discrete} approach. The testing data percentage is 5\% The composition is very successful, but the inverse is not.\relax }}{46}{figure.caption.29}}
\newlabel{graph:s4_matrix}{{3.15}{46}{$S_4$ with matrix grounding using \textbf {discrete} approach. The testing data percentage is 5\% The composition is very successful, but the inverse is not.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Units found using the \textbf  {discrete} approach in $S_4$ with matrix representation. An identity matrix was expected. Places where 1 was expected are blue, black ones are for 0.\relax }}{46}{figure.caption.30}}
\newlabel{table:s4_matrix_unit}{{3.16}{46}{Units found using the \textbf {discrete} approach in $S_4$ with matrix representation. An identity matrix was expected. Places where 1 was expected are blue, black ones are for 0.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces An extension attempt for $S_4$ with matrix representation. The blue numbers are expected to be $1$ and black ones $0$. As we see, $h^2$ is pretty much exactly what we wanted, but $h^4$ breaks down.\relax }}{47}{figure.caption.31}}
\newlabel{table:s4_matrix_half}{{3.17}{47}{An extension attempt for $S_4$ with matrix representation. The blue numbers are expected to be $1$ and black ones $0$. As we see, $h^2$ is pretty much exactly what we wanted, but $h^4$ breaks down.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces One run of \textbf  {joint} approach for the group $S_5$ using the basic grounding. We see that the successfulness is very similar to the optimisation of $S_4$.\relax }}{47}{figure.caption.32}}
\newlabel{graph:s5_joint_basic}{{3.18}{47}{One run of \textbf {joint} approach for the group $S_5$ using the basic grounding. We see that the successfulness is very similar to the optimisation of $S_4$.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces One run of \textbf  {joint} approach for the group $S_5$ using the matrix grounding. We observe an obvious drop in efficiency.\relax }}{48}{figure.caption.33}}
\newlabel{graph:s5_joint_basic}{{3.19}{48}{One run of \textbf {joint} approach for the group $S_5$ using the matrix grounding. We observe an obvious drop in efficiency.\relax }{figure.caption.33}{}}
\@setckpt{chap03}{
\setcounter{page}{49}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{19}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{34}
\setcounter{Hfootnote}{18}
\setcounter{bookmark@seq@number}{30}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@unique}{25}
\setcounter{ALC@line}{13}
\setcounter{ALC@rem}{13}
\setcounter{ALC@depth}{0}
\setcounter{thm}{6}
\setcounter{defn}{27}
\setcounter{section@level}{2}
}
